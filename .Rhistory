Find the 95% and 99% confidence intervals for the mean diamond weight. We will use t.test since we have sample data and sigma is unknown.
```{r t test for a single mean}
# Look at the t.test help file
?t.test
# First interval is 95%, which is the default if you do
# not specify conf.level
t.test(D.df$Weight)$conf.int
t.test(D.df$Weight, conf.level=0.95)$conf.int
# Second interval is 99%, which requires you to specify conf.level
t.test(D.df$Weight, conf.level=0.99)$conf.int
```
We are 95% confident that the mean weight for all diamonds represented by this sample falls between about 0.60 and 0.66 carats. We are 99% confident that the mean weight for all diamonds represented by this sample falls between 0.59 and 0.67 carats.
##### Confidence Interval for the Difference between two Population Means, Independent Samples
We will compare the mean weight for diamonds with color D compared to color I. Note that the order the variables are entered into the t.test call determines the order of subtraction. Also, we have to decide whether we want to use an un-pooled or pooled variance for the SE.
```{r t test for two means, independent samples}
# First get the right subset of data
D.CD <- filter(D.df, Color == "D")
D.CI <- filter(D.df, Color == "I")
# Look at the samples sizes and samples SDs for each group
# to inform the decision for un-pooled or pooled variance.
n.CD <- nrow(D.CD)
sd.CD <- sd(D.CD$Weight)
n.CI <- nrow(D.CI)
sd.CI <- sd(D.CI$Weight)
cbind(nCD = n.CD, SD.CD = sd.CD, nCI = n.CI, SD.CI = sd.CI)
# The sample size for color D is smallish; the two sample
# SDs are similar. This could go either way. If we pool,
# we will be giving a higher weight to the lower variance.
# Checking our ROT gives us a ratio < 3 (code shown below),
# so pooling is probably okay.
sd.CD^2/sd.CI^2
# Using un-pooled would be most conservative. At the same time
# pooling may be reasonable because we might expect the
# population variances to be similar for Weight (would we expect
# the variation in Weight to be different for different colors?).
# We will run it both ways here so you can see the difference--in
# practice, you would make a choice based on your understanding
# of diamond weights and go from there.
# First interval is 95% using an un-pooled variance, both are
# defaults
t.test(D.CD$Weight, D.CI$Weight)$conf.int
# Same interval as above but with pooled variance
t.test(D.CD$Weight, D.CI$Weight, var.equal=TRUE)$conf.int
```
The two confidence intervals are very similar. The more conservative, un-pooled variance, gives a slightly wider interval. Let's compare the df for each of the options.
```{r Looking a df for un-pooled vs. pooled}
# Same two calls from above after removing the $conf.int at
# the end to see all the t.test output
t.test(D.CD$Weight, D.CI$Weight)
t.test(D.CD$Weight, D.CI$Weight, var.equal=TRUE)
```
The un-pooled variance option has 25.583 df. The pooled variance option has 54 df. If we had done the un-pooled problem by-hand, we would have used df = min(16-1, 40-1) = 15. So a by-hand interval would have been a bit wider and more conservative. As for a single mean, you can change the confidence level by adding the conf.level argument.
We are 95% confident that the difference in mean weight for all D colored diamonds and I colored diamonds falls between about -0.28 and 0.10 carats using the un-pooled variance. We are 95% confident that the difference in mean weight for all D colored diamonds and I colored diamonds falls between about -0.27 and 0.08 carats using the pooled variance. Using either method, we see that zero is in the interval, which indicates there may be no difference between the population mean weights for D and I colored diamonds.
##### Confidence Interval for the Difference between Two Population Means, Matched Pairs
The diamond data set does not have any matched pairs data so we will need to use another data set to demonstrate this test.
The data consist of observations on 11 men with their reported height and their measured height in inches (same data used in the matched pairs lectures in Lessons 6 and 7). Find the 90% confidence interval for the difference in mean heights using measured minus reported.
```{r}
# Read in the height data
H.df <- read.csv("MensHeights.csv",header=TRUE)
# Enter measured, then reported as stated in the problem statement
# Add the paired=TRUE argument to let R know that the data are
# matched pairs. Set the conf.level to 0.90
t.test(H.df$Measured,H.df$Reported, paired=TRUE,
conf.level = 0.90)$conf.int
```
We are 90% confident that the population difference in mean heights for measured versus reported heights represented by this sample falls between about -1.12 and -0.22 inches. The interval does not contain zero suggesting that the reported and measured means may be different. In fact, it appears that the population mean measured height may be less than the population mean reported height since both endpoints are negative (we subtracted reported heights from measured heights).
##### Hypothesis Test for a Single Mean
Hypothesis tests for means use the same t.test function that was used for confidence intervals. A few differences:  1) R defaults to a null hypothesized values of zero, so you will likely need to specify a value for mu, the argument for the null value; 2) R defaults to a two-tailed test. If you want a one-tailed test you will need to use the alternative argument. The options are "two.sided" (default), "greater", or "less"; and, 3) you will not want to include the $conf.int at the end of the call because you want the hypothesis test output.
Let's test to see if the mean diamond weight exceeds 0.62 carats at the 5% significance level. When you do your assignment, you will need to include the null and alternative hypotheses.
H0:  mu = 0.62 versus Ha:  mu > 0.62
```{r}
# Include the mu and alternative arguments
t.test(D.df$Weight, mu = 0.62, alternative="greater")
```
The test gives t = 0.69 with df = 307. The p-value is 0.2451. Since p = 0.2451 > alpha = 0.05, we fail to reject the null hypothesis. There is no evidence that the true mean diamond weight exceeds 0.62 carats.
The 95% CI is reported in the output--notice how the upper bound is Inf. This is because alternative was set to greater.
##### Hypothesis Test for the Difference between Two Population Means, Independent Samples
Let's conduct a hypothesis test to determine if the mean weight for diamonds of color D is less than that for diamonds of color I at the 5% significance level. Since the subsets were already created, we do not need to recreate them but I added the code in the block below for completeness.
H0: mu[D] = mu[I] versus Ha:  mu[D] < mu[I]
In this test we will assume equal population variances for demonstration purposes. The mu argument does not need to be added in this call because the default is zero. Note that the order the variables are entered into the t.test call determines the order of subtraction.
```{r}
D.CD <- filter(D.df, Color == "D")
D.CI <- filter(D.df, Color == "I")
t.test(D.CD$Weight, D.CI$Weight, alternative="less", var.equal=TRUE)
```
The test gives t = -1.05 on 54 degrees of freedom. The p-value is 0.1497. Since p = 0.1497 > alpha = 0.05, we fail to reject the null. There is no evidence that the population mean weight of diamonds with color D is less than the mean weight for diamonds with color I.
Since this is a one-tailed test in the less than direction, the lower confidence bound is given as -Inf.
##### Hypothesis Test for the Difference between Two Population Means, Matched Pairs
We use the same matched pairs height data that was used for the confidence interval.
We will test the hypothesis that the mean reported height is different than the mean measured height at a 5% significance level.
H0: mu[d] = 0 versus Ha: mu[d] != 0, where d is reported minus measured.
In the call to t.test, we must include the paired argument. We omitted the alternative argument because we are conducting a two-tailed test and two.tailed is the default. You can always include the argument, however, to help remind you of the test you are doing. Again, remember that the order the variables are entered into the t.test call determines the order of subtraction.
```{r}
H.df <- read.csv("MensHeights.csv",header=TRUE)
t.test(H.df$Reported,H.df$Measured, paired=TRUE)
```
The test statistic is t = 2.70 on 10 degrees of freedom and the p-value is 0.0223. Since p = 0.0223 < alpha = 0.05, we reject the null. There is evidence that the reported and measured means are different. Given the t statistic is positive and that we subtracted measured from reported, it appears that the reported mean is higher than the measured mean.
#### Confidence Intervals and Hypothesis Tests for Proportions
We are writing our own functions using the standard normal procedure in order to construct confidence intervals or to run hypothesis tests for proportions.
##### Confidence Interval for a Single Proportions
We will write the function oneprop.CI to get the confidence interval for a single population proportion.
```{r oneprop.CI}
# The three arguments passed to the function are
# x = number of successes, n = total number of observations
# and conf.level. We will default conf.level to 0.95.
oneprop.CI <- function(x, n, conf.level=0.95) {
phat <- x/n
qhat <- 1 - phat
se.phat <- sqrt(phat*qhat/n)
alphaO2 <- (1 - conf.level)/2
zcrit <- abs(qnorm(alphaO2))
LB <- phat - (zcrit*(se.phat))
UB <- phat + (zcrit*(se.phat))
result <- cbind(phat = phat, Lower = LB, Upper = UB)
return(result)
}
```
Find a 90% confidence interval for the proportion of D colored diamonds. Before we proceed, we should check the large sample conditions: n x phat > 10 and n x qhat > 10. We use phat and qhat because p unknown. First check that the large sample conditions hold.
```{r Check conditions 1 prop}
tab1 <- table(D.df$Color)
sum(tab1)*(tab1[1]/sum(tab1)) #nphat
sum(tab1)*(1-(tab1[1]/sum(tab1))) #n(1-phat) = nqhat
```
The large sample conditions hold so we proceed with the 90% confidence interval.
```{r Single Prop CI}
oneprop.CI(tab1[1], sum(tab1), conf.level=0.90)
```
We are 90% confident that the true proportion of D colored diamonds in the population represented by this sample falls between about 3.1% and 7.3%.
We could also have used the binom.test function in base R to do the same thing, but the results will be slightly different because the algorithm uses an exact binomial distribution instead of a normal distribution approximation.
```{r binom.test example}
binom.test(tab1[1], sum(tab1), conf.level=0.90)$conf.int
```
As you can see, the 90% CI from binom.test is somewhat different starting in the third decimal place. So you get the interval 3.3% to 7.8% using this method.
##### Confidence Interval for Two Large Sample Population Proportions
We will write the function twoprop.CI to get the confidence interval for a the difference between two population proportions.
```{r twoprop.CI}
# The five arguments passed to the function are
# x1 = number of successes in sample 1, n1 = total number of observations
# in sample 1, x2 = number of successes in sample 2,
# n2 = total number of observations in sample 2, and
# and conf.level. We will default conf.level to 0.95.
twoprop.CI <- function(x1, n1, x2, n2, conf.level=0.95) {
phat1 <- x1/n1
qhat1 <- 1 - phat1
phat2<- x2/n2
qhat2 <- 1 - phat2
diff.phat <- phat1 - phat2
se.phat <- sqrt((phat1*qhat1/n1) + (phat2*qhat2/n2))
alphaO2 <- (1 - conf.level)/2
zcrit <- abs(qnorm(alphaO2))
LB <- diff.phat - (zcrit*(se.phat))
UB <- diff.phat + (zcrit*(se.phat))
result <- cbind(diff = diff.phat, Lower = LB, Upper = UB)
return(result)
}
```
Calculate a 98% confidence interval for the difference between the population proportions of H colored diamonds for clarity VVS1 and VVS2.
Here we are viewing the data as a 2 x 2 contingency table as follows:
Color H        Not Color H
VVS1       10              42
VVS2       15              63
Before we proceed, we should check the large sample conditions: n1 x phat1 > 10, n1 x qhat1 >10, n2 x phat2 > 10, n2 x qhat2 > 10. We will define sample 1 as VVS1 and sample 2 as VVS2. We will consider H colors successes and all other colors failures within VVS1 and VVS2.
```{r Check conditions 2 samples}
(tab2 <- table(D.df$Clarity, D.df$Color))
sum(tab2[4,]) * tab2[4,5]/sum(tab2[4,]) # n1phat1
sum(tab2[4,]) * (1- (tab2[4,5]/sum(tab2[4,]))) # n1qhat1
sum(tab2[5,]) * tab2[5,5]/sum(tab2[5,]) # n2phat2
sum(tab2[5,]) * (1- (tab2[5,5]/sum(tab2[5,]))) # n2qhat2
```
The large sample condition holds so we proceed with the confidence 98% interval.
```{r Two Prop CI}
twoprop.CI(tab2[4,5], sum(tab2[4,]), tab2[5,5], sum(tab2[5,]), conf.level=0.98)
```
We are 98% confident that the difference in the population proportion between H colored diamonds for clarity levels VVS1 and VVS2 falls between about -16.4% and 16.4%. Since zero is contained in the interval, there may be no difference between the proportions of H colored diamonds for VVS1 and VVS2.
##### Hypothesis Test for a Single Population Proportion
We will write our own function for a HT for a single population proportion.
```{r oneprop.HT}
# The three arguments passed to the function are
# x = number of successes, n = total number of observations
# and conf.level. We will default conf.level to 0.95.
oneprop.HT <- function(x, n, pmu=0, alternative="two.sided") {
phat <- x/n
se.phat <- sqrt(pmu*(1-pmu)/n)
z.score <- (phat - pmu)/se.phat
p.value <- 1 - pnorm(abs(z.score)) #upper tail
if (alternative == "two.sided") p.value = 2*p.value
result <- cbind(phat = phat, zStat = z.score, pValue = p.value)
return(result)
}
```
Let's conduct a hypothesis test to determine if the population proportion of D colored diamonds is different than 10%, using a 1% significance level. Since this is a hypothesis test, the large enough conditions that must hold are n x p0 > 10 and n x q0 > 10. Here n = 308 and p0 = 0.10. Clearly n x p0 > 10, which means n x q0 > 10. We can continue since the large sample conditions are met.
H0: p = 0.1 versus p != 0.1
```{r One prop HT}
oneprop.HT(tab1[1], sum(tab1), pmu = 0.10)
```
The p-value is 0.0049. Since p = 0.0049 < alpha = 0.01, we reject the null hypothesis. There is strong evidence that the proportion of D colored diamonds for the population represented by this sample is different than 10%.
##### Hypothesis for Two Large Sample Population Proportions
As part of R Assignment 4, you will write a function to use a z-test for a HT about the difference between two population proportions, assuming independent samples and sufficiently large sample sizes.
##### Other R functions for CIs and HTs
If you want to do a z test for means (sigma is known), then you can use z.test. This function requires the BSDA package. In the BSDA package, you will also find zsum.test and tsum.test. These functions are used for z and t tests when all you have are summary data (xbar, sd, mu, sigma, etc.). They have similar options such as turning on the equal population variance option or setting a paired argument. If you are interested in using these functions, you will need to install BSDA and add the library call to the setup chunk. Then you can use ?function_name to read details of how to call the function.
Base R also includes the functions binom.test and prop.test for proportions. As we mentioned previously, binom.test does the same thing as the oneprop.CI or oneprop.HT functions that we wrote using an exact test instead of relying on large samples for approximation. prop.test can be used for one or two sample tests for population proportions, but it uses equivalent chi-square tests, which we do not cover until Lesson 9.
##### Finding a Bootstrap CI Using R
Read in the ice cream data. See the posted data dictionary for a description of the data set. We will demonstrate finding a nonparametric bootstrap sample for the mean video score.
```{r Read ice cream data}
IC.df <- read.csv("IceCream.csv", header=TRUE)
# Look at the distribution of video game scores
hist(IC.df$Video, xlab="Video Game Score", main="")
```
The video game scores are unimodal and slightly left skewed. We know that to use the T distribution, the variable must be normally distributed in the population but we often use T if the data are unimodal and approximately symmetric with no extreme outliers. The distribution of the video scores fits this assumption and it is quite possible that the population of video game scores are normally distributed.
We will create 10000 bootstrap samples and find the 95% bootstrap interval for the mean video game score.
```{r Percentile Bootstrap mean}
# Create a vector to hold the sample means and then sample
# with replacement from the existing sample of video game scores
# calculate the mean for each sample
mean.video <- vector(length = 10000)
for (isamp in 1:10000) {
mean.video[isamp] <- mean(sample(IC.df$Video, length(IC.df$Video), replace=TRUE))
}
# sort the vector of means from smallest to largest
# then pick of the values that correspond to the
# 2.5th and 97.5th percentiles (10000*0.025=250 and
# 10000*0.975=9750)
sort.mean.video <- sort(mean.video)
LB <- sort.mean.video[250]
UB <- sort.mean.video[9750]
print(paste("Bootstrap 95% LB = ", LB))
print(paste("Bootstrap 95% UB = ", UB))
# Compare with interval from t.test
t.test(IC.df$Video)$conf.int
```
The percentile bootstrap and T distribution confidence intervals are very close. We are 95% confident that the mean video score for all students represented by this population falls between about 50.5 and 53.2.
---
title: "Stat311 Wi 21 R Assignment 4 Tutorial:  CIs and HTs for the Big-5 Parameters"
output: html_document
---
```{r setup, include=FALSE}
library(knitr)
library(dplyr)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
SHOW_SOLUTIONS = TRUE
```
Read in the Diamonds data set unless you load a saved workspace from last time.
```{r, eval=TRUE}
D.df <- read_csv ("E:/DATA SCIENCE JOBS/R-Analysis Significance Test/Diamonds_2.csv")
D.df$Color <- as.factor(D.df$Color)
D.df$Clarity <- as.factor(D.df$Clarity)
D.df$Rater <- as.factor(D.df$Rater)
```
#### CIs and HTs for Means
When working with means you use z or t statistics, depending on whether sigma is known. If sigma is unknown (most common) you should be using t; otherwise, use z. For this tutorial we will demonstrate t only. The same functions are used for confidence intervals and hypothesis tests but we will consider them separately because there are more arguments you may need include for hypothesis tests.
##### Confidence Interval for a Single Mean
Find the 95% and 99% confidence intervals for the mean diamond weight. We will use t.test since we have sample data and sigma is unknown.
```{r t test for a single mean}
# Look at the t.test help file
?t.test
# First interval is 95%, which is the default if you do
# not specify conf.level
t.test(D.df$Weight)$conf.int
t.test(D.df$Weight, conf.level=0.95)$conf.int
# Second interval is 99%, which requires you to specify conf.level
t.test(D.df$Weight, conf.level=0.99)$conf.int
```
We are 95% confident that the mean weight for all diamonds represented by this sample falls between about 0.60 and 0.66 carats. We are 99% confident that the mean weight for all diamonds represented by this sample falls between 0.59 and 0.67 carats.
##### Confidence Interval for the Difference between two Population Means, Independent Samples
We will compare the mean weight for diamonds with color D compared to color I. Note that the order the variables are entered into the t.test call determines the order of subtraction. Also, we have to decide whether we want to use an un-pooled or pooled variance for the SE.
```{r t test for two means, independent samples}
# First get the right subset of data
D.CD <- filter(D.df, Color == "D")
D.CI <- filter(D.df, Color == "I")
# Look at the samples sizes and samples SDs for each group
# to inform the decision for un-pooled or pooled variance.
n.CD <- nrow(D.CD)
sd.CD <- sd(D.CD$Weight)
n.CI <- nrow(D.CI)
sd.CI <- sd(D.CI$Weight)
cbind(nCD = n.CD, SD.CD = sd.CD, nCI = n.CI, SD.CI = sd.CI)
# The sample size for color D is smallish; the two sample
# SDs are similar. This could go either way. If we pool,
# we will be giving a higher weight to the lower variance.
# Checking our ROT gives us a ratio < 3 (code shown below),
# so pooling is probably okay.
sd.CD^2/sd.CI^2
# Using un-pooled would be most conservative. At the same time
# pooling may be reasonable because we might expect the
# population variances to be similar for Weight (would we expect
# the variation in Weight to be different for different colors?).
# We will run it both ways here so you can see the difference--in
# practice, you would make a choice based on your understanding
# of diamond weights and go from there.
# First interval is 95% using an un-pooled variance, both are
# defaults
t.test(D.CD$Weight, D.CI$Weight)$conf.int
# Same interval as above but with pooled variance
t.test(D.CD$Weight, D.CI$Weight, var.equal=TRUE)$conf.int
```
The two confidence intervals are very similar. The more conservative, un-pooled variance, gives a slightly wider interval. Let's compare the df for each of the options.
```{r Looking a df for un-pooled vs. pooled}
# Same two calls from above after removing the $conf.int at
# the end to see all the t.test output
t.test(D.CD$Weight, D.CI$Weight)
t.test(D.CD$Weight, D.CI$Weight, var.equal=TRUE)
```
The un-pooled variance option has 25.583 df. The pooled variance option has 54 df. If we had done the un-pooled problem by-hand, we would have used df = min(16-1, 40-1) = 15. So a by-hand interval would have been a bit wider and more conservative. As for a single mean, you can change the confidence level by adding the conf.level argument.
We are 95% confident that the difference in mean weight for all D colored diamonds and I colored diamonds falls between about -0.28 and 0.10 carats using the un-pooled variance. We are 95% confident that the difference in mean weight for all D colored diamonds and I colored diamonds falls between about -0.27 and 0.08 carats using the pooled variance. Using either method, we see that zero is in the interval, which indicates there may be no difference between the population mean weights for D and I colored diamonds.
##### Confidence Interval for the Difference between Two Population Means, Matched Pairs
The diamond data set does not have any matched pairs data so we will need to use another data set to demonstrate this test.
The data consist of observations on 11 men with their reported height and their measured height in inches (same data used in the matched pairs lectures in Lessons 6 and 7). Find the 90% confidence interval for the difference in mean heights using measured minus reported.
```{r}
# Read in the height data
H.df <- read.csv("MensHeights.csv",header=TRUE)
# Enter measured, then reported as stated in the problem statement
# Add the paired=TRUE argument to let R know that the data are
# matched pairs. Set the conf.level to 0.90
t.test(H.df$Measured,H.df$Reported, paired=TRUE,
conf.level = 0.90)$conf.int
```
We are 90% confident that the population difference in mean heights for measured versus reported heights represented by this sample falls between about -1.12 and -0.22 inches. The interval does not contain zero suggesting that the reported and measured means may be different. In fact, it appears that the population mean measured height may be less than the population mean reported height since both endpoints are negative (we subtracted reported heights from measured heights).
##### Hypothesis Test for a Single Mean
Hypothesis tests for means use the same t.test function that was used for confidence intervals. A few differences:  1) R defaults to a null hypothesized values of zero, so you will likely need to specify a value for mu, the argument for the null value; 2) R defaults to a two-tailed test. If you want a one-tailed test you will need to use the alternative argument. The options are "two.sided" (default), "greater", or "less"; and, 3) you will not want to include the $conf.int at the end of the call because you want the hypothesis test output.
Let's test to see if the mean diamond weight exceeds 0.62 carats at the 5% significance level. When you do your assignment, you will need to include the null and alternative hypotheses.
H0:  mu = 0.62 versus Ha:  mu > 0.62
```{r}
# Include the mu and alternative arguments
t.test(D.df$Weight, mu = 0.62, alternative="greater")
```
The test gives t = 0.69 with df = 307. The p-value is 0.2451. Since p = 0.2451 > alpha = 0.05, we fail to reject the null hypothesis. There is no evidence that the true mean diamond weight exceeds 0.62 carats.
The 95% CI is reported in the output--notice how the upper bound is Inf. This is because alternative was set to greater.
##### Hypothesis Test for the Difference between Two Population Means, Independent Samples
Let's conduct a hypothesis test to determine if the mean weight for diamonds of color D is less than that for diamonds of color I at the 5% significance level. Since the subsets were already created, we do not need to recreate them but I added the code in the block below for completeness.
H0: mu[D] = mu[I] versus Ha:  mu[D] < mu[I]
In this test we will assume equal population variances for demonstration purposes. The mu argument does not need to be added in this call because the default is zero. Note that the order the variables are entered into the t.test call determines the order of subtraction.
```{r}
D.CD <- filter(D.df, Color == "D")
D.CI <- filter(D.df, Color == "I")
t.test(D.CD$Weight, D.CI$Weight, alternative="less", var.equal=TRUE)
```
The test gives t = -1.05 on 54 degrees of freedom. The p-value is 0.1497. Since p = 0.1497 > alpha = 0.05, we fail to reject the null. There is no evidence that the population mean weight of diamonds with color D is less than the mean weight for diamonds with color I.
Since this is a one-tailed test in the less than direction, the lower confidence bound is given as -Inf.
##### Hypothesis Test for the Difference between Two Population Means, Matched Pairs
We use the same matched pairs height data that was used for the confidence interval.
We will test the hypothesis that the mean reported height is different than the mean measured height at a 5% significance level.
H0: mu[d] = 0 versus Ha: mu[d] != 0, where d is reported minus measured.
In the call to t.test, we must include the paired argument. We omitted the alternative argument because we are conducting a two-tailed test and two.tailed is the default. You can always include the argument, however, to help remind you of the test you are doing. Again, remember that the order the variables are entered into the t.test call determines the order of subtraction.
```{r}
H.df <- read.csv("MensHeights.csv",header=TRUE)
t.test(H.df$Reported,H.df$Measured, paired=TRUE)
```
The test statistic is t = 2.70 on 10 degrees of freedom and the p-value is 0.0223. Since p = 0.0223 < alpha = 0.05, we reject the null. There is evidence that the reported and measured means are different. Given the t statistic is positive and that we subtracted measured from reported, it appears that the reported mean is higher than the measured mean.
#### Confidence Intervals and Hypothesis Tests for Proportions
We are writing our own functions using the standard normal procedure in order to construct confidence intervals or to run hypothesis tests for proportions.
##### Confidence Interval for a Single Proportions
We will write the function oneprop.CI to get the confidence interval for a single population proportion.
```{r oneprop.CI}
# The three arguments passed to the function are
# x = number of successes, n = total number of observations
# and conf.level. We will default conf.level to 0.95.
oneprop.CI <- function(x, n, conf.level=0.95) {
phat <- x/n
qhat <- 1 - phat
se.phat <- sqrt(phat*qhat/n)
alphaO2 <- (1 - conf.level)/2
zcrit <- abs(qnorm(alphaO2))
LB <- phat - (zcrit*(se.phat))
UB <- phat + (zcrit*(se.phat))
result <- cbind(phat = phat, Lower = LB, Upper = UB)
return(result)
}
```
Find a 90% confidence interval for the proportion of D colored diamonds. Before we proceed, we should check the large sample conditions: n x phat > 10 and n x qhat > 10. We use phat and qhat because p unknown. First check that the large sample conditions hold.
```{r Check conditions 1 prop}
tab1 <- table(D.df$Color)
sum(tab1)*(tab1[1]/sum(tab1)) #nphat
sum(tab1)*(1-(tab1[1]/sum(tab1))) #n(1-phat) = nqhat
```
The large sample conditions hold so we proceed with the 90% confidence interval.
```{r Single Prop CI}
oneprop.CI(tab1[1], sum(tab1), conf.level=0.90)
```
We are 90% confident that the true proportion of D colored diamonds in the population represented by this sample falls between about 3.1% and 7.3%.
We could also have used the binom.test function in base R to do the same thing, but the results will be slightly different because the algorithm uses an exact binomial distribution instead of a normal distribution approximation.
```{r binom.test example}
binom.test(tab1[1], sum(tab1), conf.level=0.90)$conf.int
```
As you can see, the 90% CI from binom.test is somewhat different starting in the third decimal place. So you get the interval 3.3% to 7.8% using this method.
##### Confidence Interval for Two Large Sample Population Proportions
We will write the function twoprop.CI to get the confidence interval for a the difference between two population proportions.
```{r twoprop.CI}
# The five arguments passed to the function are
# x1 = number of successes in sample 1, n1 = total number of observations
# in sample 1, x2 = number of successes in sample 2,
# n2 = total number of observations in sample 2, and
# and conf.level. We will default conf.level to 0.95.
twoprop.CI <- function(x1, n1, x2, n2, conf.level=0.95) {
phat1 <- x1/n1
qhat1 <- 1 - phat1
phat2<- x2/n2
qhat2 <- 1 - phat2
diff.phat <- phat1 - phat2
se.phat <- sqrt((phat1*qhat1/n1) + (phat2*qhat2/n2))
alphaO2 <- (1 - conf.level)/2
zcrit <- abs(qnorm(alphaO2))
LB <- diff.phat - (zcrit*(se.phat))
UB <- diff.phat + (zcrit*(se.phat))
result <- cbind(diff = diff.phat, Lower = LB, Upper = UB)
return(result)
}
```
Calculate a 98% confidence interval for the difference between the population proportions of H colored diamonds for clarity VVS1 and VVS2.
Here we are viewing the data as a 2 x 2 contingency table as follows:
Color H        Not Color H
VVS1       10              42
VVS2       15              63
Before we proceed, we should check the large sample conditions: n1 x phat1 > 10, n1 x qhat1 >10, n2 x phat2 > 10, n2 x qhat2 > 10. We will define sample 1 as VVS1 and sample 2 as VVS2. We will consider H colors successes and all other colors failures within VVS1 and VVS2.
```{r Check conditions 2 samples}
(tab2 <- table(D.df$Clarity, D.df$Color))
sum(tab2[4,]) * tab2[4,5]/sum(tab2[4,]) # n1phat1
sum(tab2[4,]) * (1- (tab2[4,5]/sum(tab2[4,]))) # n1qhat1
sum(tab2[5,]) * tab2[5,5]/sum(tab2[5,]) # n2phat2
sum(tab2[5,]) * (1- (tab2[5,5]/sum(tab2[5,]))) # n2qhat2
```
The large sample condition holds so we proceed with the confidence 98% interval.
```{r Two Prop CI}
twoprop.CI(tab2[4,5], sum(tab2[4,]), tab2[5,5], sum(tab2[5,]), conf.level=0.98)
```
We are 98% confident that the difference in the population proportion between H colored diamonds for clarity levels VVS1 and VVS2 falls between about -16.4% and 16.4%. Since zero is contained in the interval, there may be no difference between the proportions of H colored diamonds for VVS1 and VVS2.
##### Hypothesis Test for a Single Population Proportion
We will write our own function for a HT for a single population proportion.
```{r oneprop.HT}
# The three arguments passed to the function are
# x = number of successes, n = total number of observations
# and conf.level. We will default conf.level to 0.95.
oneprop.HT <- function(x, n, pmu=0, alternative="two.sided") {
phat <- x/n
se.phat <- sqrt(pmu*(1-pmu)/n)
z.score <- (phat - pmu)/se.phat
p.value <- 1 - pnorm(abs(z.score)) #upper tail
if (alternative == "two.sided") p.value = 2*p.value
result <- cbind(phat = phat, zStat = z.score, pValue = p.value)
return(result)
}
```
Let's conduct a hypothesis test to determine if the population proportion of D colored diamonds is different than 10%, using a 1% significance level. Since this is a hypothesis test, the large enough conditions that must hold are n x p0 > 10 and n x q0 > 10. Here n = 308 and p0 = 0.10. Clearly n x p0 > 10, which means n x q0 > 10. We can continue since the large sample conditions are met.
H0: p = 0.1 versus p != 0.1
```{r One prop HT}
oneprop.HT(tab1[1], sum(tab1), pmu = 0.10)
```
The p-value is 0.0049. Since p = 0.0049 < alpha = 0.01, we reject the null hypothesis. There is strong evidence that the proportion of D colored diamonds for the population represented by this sample is different than 10%.
##### Hypothesis for Two Large Sample Population Proportions
As part of R Assignment 4, you will write a function to use a z-test for a HT about the difference between two population proportions, assuming independent samples and sufficiently large sample sizes.
##### Other R functions for CIs and HTs
If you want to do a z test for means (sigma is known), then you can use z.test. This function requires the BSDA package. In the BSDA package, you will also find zsum.test and tsum.test. These functions are used for z and t tests when all you have are summary data (xbar, sd, mu, sigma, etc.). They have similar options such as turning on the equal population variance option or setting a paired argument. If you are interested in using these functions, you will need to install BSDA and add the library call to the setup chunk. Then you can use ?function_name to read details of how to call the function.
Base R also includes the functions binom.test and prop.test for proportions. As we mentioned previously, binom.test does the same thing as the oneprop.CI or oneprop.HT functions that we wrote using an exact test instead of relying on large samples for approximation. prop.test can be used for one or two sample tests for population proportions, but it uses equivalent chi-square tests, which we do not cover until Lesson 9.
##### Finding a Bootstrap CI Using R
Read in the ice cream data. See the posted data dictionary for a description of the data set. We will demonstrate finding a nonparametric bootstrap sample for the mean video score.
```{r Read ice cream data}
IC.df <- read.csv("IceCream.csv", header=TRUE)
# Look at the distribution of video game scores
hist(IC.df$Video, xlab="Video Game Score", main="")
```
The video game scores are unimodal and slightly left skewed. We know that to use the T distribution, the variable must be normally distributed in the population but we often use T if the data are unimodal and approximately symmetric with no extreme outliers. The distribution of the video scores fits this assumption and it is quite possible that the population of video game scores are normally distributed.
We will create 10000 bootstrap samples and find the 95% bootstrap interval for the mean video game score.
```{r Percentile Bootstrap mean}
# Create a vector to hold the sample means and then sample
# with replacement from the existing sample of video game scores
# calculate the mean for each sample
mean.video <- vector(length = 10000)
for (isamp in 1:10000) {
mean.video[isamp] <- mean(sample(IC.df$Video, length(IC.df$Video), replace=TRUE))
}
# sort the vector of means from smallest to largest
# then pick of the values that correspond to the
# 2.5th and 97.5th percentiles (10000*0.025=250 and
# 10000*0.975=9750)
sort.mean.video <- sort(mean.video)
LB <- sort.mean.video[250]
UB <- sort.mean.video[9750]
print(paste("Bootstrap 95% LB = ", LB))
print(paste("Bootstrap 95% UB = ", UB))
# Compare with interval from t.test
t.test(IC.df$Video)$conf.int
```
The percentile bootstrap and T distribution confidence intervals are very close. We are 95% confident that the mean video score for all students represented by this population falls between about 50.5 and 53.2.
---
title: "Stat311 Sp 21 R Assignment 4 Solutions"
author: ""
date: ""
output: html_document
---
```{r setup, include=FALSE}
library(knitr)
library(dplyr)
library(ggplot2)
library(BSDA)
source("Functions.R")
knitr::opts_chunk$set(echo = TRUE)
SHOW_SOLUTIONS = TRUE
```
Read in the ice cream, birthweight, and cholesterol data sets.
```{r, eval=TRUE}
IC.df <- read.csv("IceCream.csv", header=TRUE, as.is=TRUE)
IC.df$Sex <- as.factor(IC.df$Sex)
IC.df$Flavor <- as.factor(IC.df$Flavor)
#
BW.df <- read.csv("BirthWeight.csv", header=TRUE, as.is=TRUE)
BW.df$Smoker <- as.factor(BW.df$Smoker)
BW.df$BirthWt <- as.factor(BW.df$BirthWt)
BW.df$MAgeGT35 <- as.factor(BW.df$MAgeGT35)
#
C.df <- read.csv("Cholesterol.csv", header=TRUE, as.is=TRUE)
C.df$Cereal <- as.factor(C.df$Cereal)
```
